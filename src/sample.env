# ip address, make sure to include the exposed port of vLLM
# also, if vLLM is hosted on local host, you can make your IP: host.docker.internal 
IP_ADDRESS="random.ip:8000"

# include the model name that the vLLM docker container is serving
model="mistralai/Mistral-7B-v0.1"

# this is only used for  the caption of the test
model_name="Mistral-7B-v0.1"

# maximum number of tokens and temperature
max_tokens=1000
temperature=0.0


# Fluent Bit HTTP input
host=fluent-bit
port=9880

# Fluent Bit tag --> I recommend not changing this
tag=vllm

# Seconds between requests
interval=0.5

# Number of requests to send
num_requests=100

# name of dataset
dataset="evol-codealpaca-v1"

# For any additional notes that you want to add to the test
caption=""